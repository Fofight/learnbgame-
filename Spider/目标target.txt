断点续爬功能，内容去重，实时应付反爬，异常实时检测和处理，及时反馈，

智能去重，如已经爬过的id爬虫不会再次爬取，也就是说爬虫会记住爬取过的id，每个id获取完了所有内容之后会被标记为已经爬取；

延时策略，合理设置阈值，

通过大量的异常检测和处理，捕获所有的解析和抓取异常。编写解析代码来获取足够全面的信息，及时反馈

分布式爬取，增量式抓取，

书写详细代码注释，方便阅读，增加复用性，以便二次开发，

爬取内容：用户信息爬取，指定关键字搜索结果增量爬取，指定用户主页所有微博抓取，评论抓取和转发关系抓取等，

分布式，通用性，多线程，大规模式爬虫

代码的稳定性（容错性）

看起来像人类用户

自动的数据获取

爬取基本数据已经不是问题了，你的瓶颈会集中到爬取海量数据的效率，这个时候，相信你会很自然地接触到一个很厉害的名字，分布式爬虫。
分布式这个东西，听起来很恐怖，但其实就是利用多线程的原理让多个爬虫同时工作，通常会有scrapy+数据库+redis这这三个工具。

当你能够写分布式的爬虫的时候，那么你可以去尝试打造一些基本的架构了，实现一些更加自动的数据获取









