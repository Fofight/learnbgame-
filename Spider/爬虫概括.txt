爬虫的模式又可以用scrapy的parse->yield item->pipeline流程来概括

解析函数，处理页面,pipeline，数据处理。



爬虫获取数据的过程一般来说是post请求，get数据，并对数据进行处理的过程

爬取的过程又可概括为url-->response-->bytes-->str-->提取数据

大部分爬虫都是按“发送请求-->获得页面-->解析页面-->抽取并存储内容”这样的流程来进行，这其实也是模拟了我们使用浏览器获取网页信息的过程。
简单来说，我们向服务器发送请求后，会得到返回的页面，通过解析页面之后，我们可以抽取我们想要的那部分信息，并存储在指定的文档或数据库中。


可以简单了解HTTP协议及网页基础知识，比如POST\GET、HTML、CSS、JS，简单了解即可，不需要系统地学习。

python中爬虫相关的包很多：urllib，requests，bs4,scrapy，pyspider等，

requests，urllib，等包可以负责连接网站，返回网页，xpath，lxml等用于解析网页，便于抽取数据。

逐渐深入之后，你会发现爬虫的基本套路都差不多，一般的静态网站根本不在话下，如果需要爬取异步加载的网站，可以学习浏览器抓包分析真实请求，或学习selenium来实现自动化。

这个过程中你还要了解一些python的基础知识：
文件读写操作：用来读取参数，保存爬下来的内容
list（列表），dict（字典）：用来序列化爬取的数据
条件判断（if/else）：解决爬虫中是否执行
循环和迭代（for......while):用来循环爬虫步骤

爬回来的数据可以直接用文档形式存在本地，也可存入数据库中。

开始数据量不大的时候i，你可以直接通过python的语法或pandas的方法将数据存为csv这样的文件，当然你可能发现爬回来的数据并不是干净的，可能会有缺失、错误等，你还需要对数据进行清理，可以学习pandas包的基本用法来做数据的预处理，得到更干净的数据






