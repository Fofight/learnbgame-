在实际的爬虫过程中，经常会遇到一些网站的反爬措施：

1.通过请求头识别用户,user-agent访问限制。经典python爬虫在使用urllib标准库时，都会发送请求头{User-Agent：Python-urllib/3.4}

2.被网站封IP。


3.需要用户登录才能获得浏览相应内容的权限


4.通过用户的相关访问频率来判别是否为爬虫行为。

5.蜜罐
	如果网络表单的一个字段通过css设置成对用户不可见，那么可以认为普通用户访问网站的时候不能填写这个字段，因为它没有显示在浏览器上。如果这个字段被填写了，就有可能是机器人干的，因此这个提交会实效。访问者如果访问了网站的一个“隐含”内容，就会触发服务器脚本封杀这个用户的IP地址，把这个用户提出网站，或者采取其他措施禁止这个用户接入网站。

6.验证码
	有些网站通常在登录时通常都会通过使用验证码的方式来进行反爬虫工作

7.动态加载，异步加载。

遇到这些反爬虫的手段，需要一些技巧来应对：

关于情况1,通常可通过伪装成浏览器的访问的方法,如设置user-agent

关于情况2,可以通过使用代理IP代理池的方式进行访问

关于情况3,通常通过获取并利用cookie的方式进行验证或模拟人类用户的访问方式使用selenium等模块进行模拟登录

关于情况4，可以适当地进行延时设置，访问频率控制，使爬虫看起来更像人

关于情况5，可以通过selenium模块模拟人进行爬虫相关操作

关于情况6，可以通过验证码的OCR处理，当然，目前有一些打码平台提供相应的api进行识别验证码操作,如云打码(http://www.yundama.com/)，但需要收取一定的费用,
运用云打码的两个实例：
	https://github.com/LiuXingMing/SinaSpider
	https://github.com/SpiderClub/weibospider





破解反爬虫机制的方法总的来说就是让网络机器人看起来像人类用户



问题检查列表

• 首先，如果从网络服务器收到的页面是空白的，缺少信息，或其遇到其他不符合预期的情况（或者不是你在浏览器上看到的内容），有可能是因为网站创建页面的JavaScript执行有问题。
• 如果准备向网站提交表单或发出POST 请求，记得检查一下页面的内容，看看想提交的每个字段是不是都已经填好，而且格式也正确。
• 如果已经登录网站却不能保持登录状态，或者网站上出现了其他的“登录状态”异常，检查cookie。确认在加载每个页面时cookie 都被正确调用，而且cookie 在每次发起请求时都发送到了网站上。
• 如果在客户端遇到了HTTP 错误，尤其是403 禁止访问错误，这可能说明网站已经把当前IP 当作机器人了，不再接受你的任何请求。要么等待IP 地址从网站黑名单里移除，要么就换个IP 地址。如果确定自己并没有被封杀，那么再检查下面的内容。
1. 确认爬虫在网站上的速度不是特别快。快速采集是一种恶习，会对服务器造成沉重的负担，也是IP 被网站列入黑名单的首要原因。给爬虫增加延迟，让它们在夜深人静的时候运行。切记：匆匆忙忙写程序或收集数据都是拙劣项目管理的表现；应该提前做好计划，避免临阵慌乱。
2. 修改你的请求头！有些网站会封杀任何声称自己是爬虫的访问者。
3. 确认没有点击或访问任何人类用户通常不能点击或接入的信息
